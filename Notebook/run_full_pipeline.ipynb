{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import os\n",
        "os.environ.setdefault('KMP_DUPLICATE_LIB_OK', 'TRUE')\n",
        "\n",
        "import csv\n",
        "import json\n",
        "import math\n",
        "import random\n",
        "import subprocess\n",
        "import sys\n",
        "from collections import Counter\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "\n",
        "PROJECT_ROOT = Path.cwd().resolve().parent if Path.cwd().name.lower() == 'notebook' else Path.cwd().resolve()\n",
        "if str(PROJECT_ROOT) not in sys.path:\n",
        "    sys.path.insert(0, str(PROJECT_ROOT))\n",
        "\n",
        "from Main.config import ensure_output_dirs, get_default_config\n",
        "from Main.dataset import build_dataloaders\n",
        "from Main.evaluate import evaluate_model, save_confusion_matrix_plot\n",
        "from Main.interpret import run_band_importance\n",
        "from Main.model import build_model\n",
        "from Main.train import build_class_weights, train_one_epoch\n",
        "\n",
        "print('Project root:', PROJECT_ROOT)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def set_seed(seed: int = 42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "raw_dir = PROJECT_ROOT / 'Data' / 'raw'\n",
        "processed_dir = PROJECT_ROOT / 'Data' / 'processed'\n",
        "metadata_path = PROJECT_ROOT / 'Data' / 'metadata.csv'\n",
        "processed_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def _make_epoch_signal(label: int, samples: int = 3000, fs: int = 100) -> np.ndarray:\n",
        "    t = np.arange(samples) / fs\n",
        "    # \n",
        "    if label == 0:      # N1 theta\n",
        "        sig = 0.8 * np.sin(2 * np.pi * 6 * t)\n",
        "    elif label == 1:    # N2 sigma + theta\n",
        "        sig = 0.7 * np.sin(2 * np.pi * 13 * t) + 0.4 * np.sin(2 * np.pi * 7 * t)\n",
        "    elif label == 2:    # N3 delta\n",
        "        sig = 1.2 * np.sin(2 * np.pi * 2 * t)\n",
        "    else:               # REM alpha + beta\n",
        "        sig = 0.6 * np.sin(2 * np.pi * 10 * t) + 0.3 * np.sin(2 * np.pi * 20 * t)\n",
        "    noise = 0.25 * np.random.randn(samples)\n",
        "    return (sig + noise).astype(np.float32)\n",
        "\n",
        "def generate_synthetic_dataset():\n",
        "    subjects = [f'S{i:02d}' for i in range(1, 13)]\n",
        "    split_map = {}\n",
        "    for i, s in enumerate(subjects):\n",
        "        if i < 8:\n",
        "            split_map[s] = 'train'\n",
        "        elif i < 10:\n",
        "            split_map[s] = 'val'\n",
        "        else:\n",
        "            split_map[s] = 'test'\n",
        "\n",
        "    rows = []\n",
        "    class_counter = Counter()\n",
        "    for subject_id in subjects:\n",
        "        for rec_idx in range(2):\n",
        "            record_id = f'{subject_id}_R{rec_idx+1}'\n",
        "            n_epochs = 80\n",
        "            probs = np.array([0.2, 0.35, 0.25, 0.2], dtype=np.float64)\n",
        "            labels = np.random.choice(np.arange(4), size=n_epochs, p=probs).astype(np.int64)\n",
        "\n",
        "            eeg = np.stack([_make_epoch_signal(int(y)) for y in labels], axis=0)[:, None, :]\n",
        "            eeg = (eeg - eeg.mean(axis=-1, keepdims=True)) / (eeg.std(axis=-1, keepdims=True) + 1e-6)\n",
        "\n",
        "            eeg_path = processed_dir / f'{record_id}_eeg.npy'\n",
        "            label_path = processed_dir / f'{record_id}_label.npy'\n",
        "            np.save(eeg_path, eeg.astype(np.float32))\n",
        "            np.save(label_path, labels.astype(np.int64))\n",
        "\n",
        "            class_counter.update(labels.tolist())\n",
        "            rows.append({\n",
        "                'subject_id': subject_id,\n",
        "                'record_id': record_id,\n",
        "                'eeg_path': str(eeg_path),\n",
        "                'label_path': str(label_path),\n",
        "                'split': split_map[subject_id],\n",
        "            })\n",
        "\n",
        "    with metadata_path.open('w', encoding='utf-8', newline='') as f:\n",
        "        writer = csv.DictWriter(f, fieldnames=['subject_id', 'record_id', 'eeg_path', 'label_path', 'split'])\n",
        "        writer.writeheader()\n",
        "        writer.writerows(rows)\n",
        "\n",
        "    return rows, class_counter\n",
        "\n",
        "psg_files = list(raw_dir.rglob('*PSG.edf')) if raw_dir.exists() else []\n",
        "hyp_files = list(raw_dir.rglob('*Hypnogram.edf')) if raw_dir.exists() else []\n",
        "\n",
        "if psg_files and hyp_files:\n",
        "    print(f'Found raw Sleep-EDF files in {raw_dir}, running preprocess script...')\n",
        "    cmd = [\n",
        "        sys.executable,\n",
        "        str(PROJECT_ROOT / 'Main' / 'preprocess.py'),\n",
        "        '--raw-dir', str(raw_dir),\n",
        "        '--processed-dir', str(processed_dir),\n",
        "        '--metadata-path', str(metadata_path),\n",
        "        '--channel', 'Fpz-Cz',\n",
        "    ]\n",
        "    subprocess.run(cmd, check=True)\n",
        "    print('Preprocess finished:', metadata_path)\n",
        "else:\n",
        "    rows, class_counter = generate_synthetic_dataset()\n",
        "    print('No raw EDF found. Generated synthetic dataset for full-pipeline smoke test.')\n",
        "    print('Records:', len(rows), 'Class distribution:', dict(class_counter))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cfg = get_default_config()\n",
        "cfg.data.metadata_path = metadata_path\n",
        "cfg.data.processed_root = processed_dir\n",
        "cfg.data.context_window = 5\n",
        "\n",
        "cfg.model.max_seq_len = cfg.data.context_window\n",
        "cfg.model.in_channels = 1\n",
        "\n",
        "cfg.train.epochs = 4\n",
        "cfg.train.batch_size = 64\n",
        "cfg.train.num_workers = 0\n",
        "cfg.train.lr = 1e-3\n",
        "cfg.train.early_stop_patience = 4\n",
        "\n",
        "ensure_output_dirs(cfg)\n",
        "train_ds, val_ds, test_ds, train_loader, val_loader, test_loader = build_dataloaders(cfg)\n",
        "\n",
        "print('Train/Val/Test samples:', len(train_ds), len(val_ds), len(test_ds))\n",
        "print('Train class counts:', dict(train_ds.class_counts))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = build_model(cfg).to(device)\n",
        "\n",
        "class_weights = None\n",
        "if cfg.train.use_class_weights:\n",
        "    class_weights = build_class_weights(train_ds.class_counts, cfg.model.num_classes, device)\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
        "optimizer = AdamW(model.parameters(), lr=cfg.train.lr, weight_decay=cfg.train.weight_decay)\n",
        "scheduler = CosineAnnealingLR(optimizer, T_max=max(cfg.train.epochs, 1))\n",
        "\n",
        "best_val_f1 = -1.0\n",
        "history = []\n",
        "\n",
        "for epoch in range(1, cfg.train.epochs + 1):\n",
        "    train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
        "    val_metrics = evaluate_model(model, val_loader, device, cfg.model.num_classes, criterion=criterion)\n",
        "    scheduler.step()\n",
        "\n",
        "    history.append({\n",
        "        'epoch': epoch,\n",
        "        'train_loss': train_loss,\n",
        "        'val_loss': val_metrics['loss'],\n",
        "        'val_accuracy': val_metrics['accuracy'],\n",
        "        'val_macro_f1': val_metrics['macro_f1'],\n",
        "    })\n",
        "\n",
        "    if float(val_metrics['macro_f1']) > best_val_f1:\n",
        "        best_val_f1 = float(val_metrics['macro_f1'])\n",
        "        torch.save({'model_state_dict': model.state_dict()}, cfg.result.best_ckpt_path)\n",
        "\n",
        "    print(f\"[Epoch {epoch}] train_loss={train_loss:.4f} val_macro_f1={val_metrics['macro_f1']:.4f}\")\n",
        "\n",
        "print('Best val macro-F1:', best_val_f1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ckpt = torch.load(cfg.result.best_ckpt_path, map_location=device)\n",
        "model.load_state_dict(ckpt['model_state_dict'])\n",
        "\n",
        "test_metrics = evaluate_model(model, test_loader, device, cfg.model.num_classes, criterion=criterion)\n",
        "class_names = [k for k, _ in sorted(cfg.data.label_map.items(), key=lambda kv: kv[1])]\n",
        "save_confusion_matrix_plot(test_metrics['confusion_matrix'], class_names, cfg.result.confusion_matrix_path)\n",
        "\n",
        "summary = {\n",
        "    'history': history,\n",
        "    'test': {\n",
        "        'loss': test_metrics['loss'],\n",
        "        'accuracy': test_metrics['accuracy'],\n",
        "        'macro_f1': test_metrics['macro_f1'],\n",
        "        'cohen_kappa': test_metrics['cohen_kappa'],\n",
        "        'per_class': test_metrics['per_class'],\n",
        "        'confusion_matrix': test_metrics['confusion_matrix'].tolist(),\n",
        "    }\n",
        "}\n",
        "\n",
        "cfg.result.metrics_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "with cfg.result.metrics_path.open('w', encoding='utf-8') as f:\n",
        "    json.dump(summary, f, indent=2)\n",
        "\n",
        "print(json.dumps(summary['test'], indent=2))\n",
        "print('Saved metrics to', cfg.result.metrics_path)\n",
        "print('Saved confusion matrix to', cfg.result.confusion_matrix_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rows = run_band_importance(\n",
        "    model=model,\n",
        "    data_loader=test_loader,\n",
        "    device=device,\n",
        "    num_classes=cfg.model.num_classes,\n",
        "    sampling_rate=cfg.data.sampling_rate,\n",
        "    output_csv=cfg.result.band_importance_path,\n",
        ")\n",
        "\n",
        "print('Band importance saved to', cfg.result.band_importance_path)\n",
        "for row in rows:\n",
        "    print(row)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
